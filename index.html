<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <title>ICML 2020 Workshop</title>

    <!-- css -->
<link rel="icon" href="header.jpg" type="image/gif">

    <link rel="stylesheet" href="bower_components/bootstrap/dist/css/bootstrap.min.css">
    <link rel="stylesheet" href="bower_components/ionicons/css/ionicons.min.css">
    <link rel="stylesheet" href="assets/css/main.css">
</head>
<body data-spy="scroll" data-target="#site-nav">
    <nav id="site-nav" class="navbar navbar-fixed-top navbar-custom">
        <div class="container">
            <div class="navbar-header">

                <!-- logo -->
                <div class="site-branding">
                <p>ICML 2020 Workshop</p>
                </div>

                <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar-items" aria-expanded="false">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>

            </div><!-- /.navbar-header -->

            <div class="collapse navbar-collapse" id="navbar-items">
                <ul class="nav navbar-nav navbar-right">

                    <!-- navigation menu -->
                    <li class="active"><a data-scroll href="#overview">Overview</a></li>
                    <li><a data-scroll href="#speakers">Invited Speakers</a></li>           
                    <li><a data-scroll href="#contribution">Call for Papers</a></li>
                    <li><a data-scroll href="#schedule">Schedule</a></li> 
                    <li><a data-scroll href="#people">People</a></li> 
                    <li><a data-scroll href="#references">References</a></li>                  

                    <!-- <li><a data-scroll href="#">Sponsorship</a></li>
                    <li><a data-scroll href="#peop">Organizers</a></li>
               <li><a data-scroll href="#photos">Photos</a></li>-->
                
                </ul>
            </div>
        </div><!-- /.container -->
    </nav>

    <header id="site-header" class="site-header valign-center"> 
        <div class="intro">

            <!--<h2>25 April, 2015 / Townhall California</h2>-->
            
            <h1>Bridge Between Perception and Reasoning:</h1>
            <h1>Graph Neural Networks & Beyond</h1>
            
            <p>ICML 2020 Workshop</p>
            
            <!--<a class="btn btn-white" data-scroll href="#registration">Register Now</a>-->
        
        </div>
    </header>

    <section id="overview" class="section overview">
        <div class="container">
            <div class="row">

                <div class="col-sm-12">
                    <h3 class="section-title">Overview</h3>

                    <p> Deep learning has achieved great success in a variety of tasks such as recognizing objects in images, predicting the sentiment of sentences, or image/speech synthesis by training on a large-amount of data. However, most existing success are mainly focusing on perceptual tasks, which is also known as System I intelligence. In real world, many complicated tasks, such as autonomous driving, public policy decision making, and multi-hop question answering, require understanding the relationship between high-level variables in the data to perform logical reasoning, which is known as System II intelligence. Integrating system I and II intelligence lies in the core of artificial intelligence and machine learning.   </p>

                    <p>Graph is an important structure for System II intelligence, with the universal representation ability to capture the relationship between different variables, and support interpretability, causality, and transferability / inductive generalization. Traditional logic and symbolic reasoning over graphs has relied on methods and tools which are very different from deep learning models, such Prolog language, SMT solvers, constrained optimization and discrete algorithms. Is such a methodology separation between System I and System II intelligence necessary? How to build a flexible, effective and efficient bridge to smoothly connect these two systems, and create higher order artificial intelligence? </p>

                    <p>Graph neural networks, have emerged as the tool of choice for graph representation learning, which has led to impressive progress in many classification and regression problems such as chemical synthesis, 3D-vision, recommender systems and social network analysis. However, prediction and classification tasks can be very different from logic/symbolic reasoning.</p>

                    <p>Bits and pieces of evidence can be gleaned from recent literature, suggesting graph neural networks may be a general tool to make such a connection. For example, (Battaglia et al., 2018; Barceĺó et al., 2019) viewed graph neural networks as tools to incorporate explicitly logic reasoning bias. (Kipf et al., 2018) used graph neural network to reason about interacting systems, (Yoon et al., 2018; Zhang et al., 2020) used neural networks for logic and probabilistic inference, (Hudson &Manning, 2019; Hu et al., 2019) used graph neural networks for reasoning on scene graphs for visual question reasoning, (Qu & Tang, 2019) studied reasoning on knowledge graphs with graph neural networks, and (Khalil et al., 2017; Xu et al.,2018; Velickovic et al., 2019; Sato et al., 2019) used graph neural networks for discrete graph algorithms. However, there can still be a long way to go for a satisfactory and definite answers on the ability of graph neural networks for automatically discovering logic rules, and conducting long-range multi-step complex reasoning in combination with perception inputssuch as language, vision, spatial and temporal variation.</p>

                    <p> <b>Can graph neural networks be the key bridge to connect System I and System II intelligence? Are there other more flexible, effective and efficient alternatives?</b> For instance, (Wang et al., 2019) combined max satisfiability solver withdeep learning, (Manhaeve et al., 2018) combined directed graphical and Problog with deep learning, (Arseny Skryagin,2020) combined sum product network with deep learning, (Silver et al., 2019; Alet et al., 2019) combined logic reasoning with reinforcement learning. How do these alternative methods compare with graph neural networks for being a bridge? </p>

                    <p>The goal of this workshop is to bring researchers from previously separate fields, such as deep learning, logic/symbolic reasoning, statistical relational learning, and graph algorithms, into a common roof to discuss this potential interface and integration between System I and System intelligence. By providing a venue for the confluence of new advances in theoretical foundations, models and algorithms, as well as empirical discoveries, new benchmarks and impactful applications,</p>

                    <p>we hope this can shed light on the bridge to higher order intelligence, and spark new ideas along this direction. The topics discussed in this workshop will include but are not limited to:  </p>
                    
                    <ul class="list-arrow-right">
                        <li>Deep learning and graph neural networks for logic reasoning, knowledge graphs and relational data.</li>
                        <li>Deep relational and graph reasoning in computer vision.</li>
                        <li>Deep learning and graph neural networks for multi-hop reasoning in natural language and text corpora.</li>
                        <li>Deep learning for statistical relational modeling (e.g., Bayes networks, Markov networks and causal models).</li>
                        <li>Deep learning for graph and symbolic  algorithms (e.g., combinatorial and iterative algorithms).</li>
                        <li>Deep learning for induction of structures, such as logic and mathematical formulas and relational patterns.</li>
                        <li>Theoretical foundation of graph neural networks for logic reasoning and graph algorithms.</li>
                        <li>Applications in different fields such as computer vision, natural language processing, healthcare and other sciences.</li>
                        <li>Benchmark data sets and open source libraries.</li>
                        <li>Other mechanisms such as attention and consciousness.</li>               
                    </ul>

                </div>
            </div>
        </div>
    </section>

    <section id="speakers" class="section speakers">
        <div class="container">
            <div class="row">
                <div class="col-md-12">
                    <h3 class="section-title">Speakers</h3>
                </div>
            </div>

            <div class="row">
                <div class="col-md-4">
                    <div class="speaker">

                        <figure>
                            <a href="https://mila.quebec/en/yoshua-bengio/"><img alt=""  src="https://mila.quebec/wp-content/uploads/2018/11/Yoshua-Bengio-2018-e1554338670290-500x550.jpg" height="200px"></a>
                        </figure>

                        <h4>Yoshua Bengio</h4>

                        <p>University of Montreal & Mila</p>

                        <p>System 1 and 2 reasoning</p>

                    </div><!-- /.speaker -->
                </div><!-- /.col-md-4 -->

                <div class="col-md-4">
                    <div class="speaker">

                        <figure>
                            <a href="https://scholar.google.com/citations?user=nQ7Ij30AAAAJ&hl=en"><img alt=""  src="https://scholar.googleusercontent.com/citations?view_op=medium_photo&user=nQ7Ij30AAAAJ&citpid=2" height="200px"></a>
                        </figure>

                        <h4>Peter Battaglia</h4>

                        <p>DeepMind</p>

                        <p>Graph neural networks and relational learning</p>

                    </div><!-- /.speaker -->
                </div><!-- /.col-md-4 -->

                <div class="col-md-4">
                    <div class="speaker">

                        <figure>
                            <a href="http://zicokolter.com/"><img alt=""  src="http://zicokolter.com/img/zicokolter.jpg" height="200px"></a>
                        </figure>

                        <h4>Zico Kolter</h4>

                        <p>CMU</p>

                        <p>Deep learning and logic constraint satifiability</p>

                    </div><!-- /.speaker -->
                </div><!-- /.col-md-4 -->
            </div><!-- /.row -->

            <div class="row">
                <div class="col-md-4">
                    <div class="speaker">

                        <figure>
                            <a href="http://128.30.27.50:8000/"><img alt=""  src="http://128.30.27.50:8000/images/FerranAletNov2019Wall.jpg" height="200px"></a>
                        </figure>

                        <h4>Ferran Alet</h4>

                        <p>MIT</p>

                        <p>Structured reasoning and planning</p>

                    </div><!-- /.speaker -->
                </div><!-- /.col-md-4 -->

                <div class="col-md-4">
                    <div class="speaker">

                        <figure>
                            <a href="http://people.csail.mit.edu/tommi/"><img alt=""  src="http://people.csail.mit.edu/tommi/tommi.png" height="200px"></a>
                        </figure>

                        <h4>Tommi Jaakkola</h4>

                        <p>MIT</p>

                        <p>GNNs for Chemical Reasoning</p>

                    </div><!-- /.speaker -->
                </div><!-- /.col-md-4 -->

                <div class="col-md-4">
                    <div class="speaker">

                        <figure>
                            <a href="https://people.cs.kuleuven.be/~luc.deraedt/"><img alt=""  src="https://people.cs.kuleuven.be/~luc.deraedt/images/deraedt2.jpg" height="200px"></a>
                        </figure>

                        <h4>Luc De Raedt</h4>

                        <p>KU Leuven</p>

                        <p>Deep learning and ProbLog</p>

                    </div><!-- /.speaker -->
                </div><!-- /.col-md-4 -->
            </div><!-- /.row -->

            <div class="row">
                <div class="col-md-4">
                    <div class="speaker">

                        <figure>
                            <a href="https://ml-research.github.io/people/kkersting/index.html"><img alt=""  src="https://scholar.googleusercontent.com/citations?view_op=medium_photo&user=QY-earAAAAAJ&citpid=9" height="200px"></a>
                        </figure>

                        <h4>Kristian Kersting</h4>

                        <p>TU Darmstadt</p>

                        <p>Deep learning and Sum-Product logic</p>

                    </div><!-- /.speaker -->
                </div><!-- /.col-md-4 -->

            </div><!-- /.row -->

        </div>
    </section>

    <section id="contribution" class="section overview">
        <div class="container">
            <div class="row">
                <div class="col-md-12">
                    <h3 class="section-title">Call for Papers</h3>
                    <p>TBD</p>
                </div>
            </div>
        </div>
    </section>

    <section id="schedule" class="section schedule">
        <div class="container">
            <div class="row">
                <div class="col-md-12">
                    <h3 class="section-title">Tentative Schedule</h3>
                    08:45 AM  &nbsp;&nbsp;&nbsp;&nbsp;  Opening remarks  </br>
                    09:00 AM  &nbsp;&nbsp;&nbsp;&nbsp;  Keynote Talk </br>
                    09:30 AM  &nbsp;&nbsp;&nbsp;&nbsp;  Contributed Talk (1) </br>
                    09:45 AM  &nbsp;&nbsp;&nbsp;&nbsp;  Poster spotlights #1 (Spotlight talks) </br>
                    10:00 AM  &nbsp;&nbsp;&nbsp;&nbsp;  Morning poster session and coffee break (Posters) </br>
                    11:00 AM  &nbsp;&nbsp;&nbsp;&nbsp;  Invited Talk 1 </br>
                    11:20 AM  &nbsp;&nbsp;&nbsp;&nbsp;  Invited Talk 2 </br>
                    11:40 PM  &nbsp;&nbsp;&nbsp;&nbsp;  Contributed Talk (2) </br>
                    11:55 PM  &nbsp;&nbsp;&nbsp;&nbsp;  Poster spotlights #2 (Spotlight talks) </br>
                    12:15 PM  &nbsp;&nbsp;&nbsp;&nbsp;  Lunch break (Break) </br>
                    01:45 PM  &nbsp;&nbsp;&nbsp;&nbsp;  Invited Talk  3 </br>
                    02:05 PM  &nbsp;&nbsp;&nbsp;&nbsp;  Invited Talk  4 </br>
                    02:25 PM  &nbsp;&nbsp;&nbsp;&nbsp;  Contributed Talk (3)  </br>
                    02:40 PM  &nbsp;&nbsp;&nbsp;&nbsp;  Poster spotlights #3 (Spotlight talks) </br>
                    03:00 PM  &nbsp;&nbsp;&nbsp;&nbsp;  Invited Talk  5 </br>
                    03:20 PM  &nbsp;&nbsp;&nbsp;&nbsp;  Invited Talk 6 </br>
                    03:40 PM  &nbsp;&nbsp;&nbsp;&nbsp;  Afternoon poster session and coffee break (Posters) </br>
                    04:30 PM  &nbsp;&nbsp;&nbsp;&nbsp;  Invited Talk  7 </br>
                    04:50 PM  &nbsp;&nbsp;&nbsp;&nbsp;  Invited Talk  8 </br>
                    05:10 PM  &nbsp;&nbsp;&nbsp;&nbsp;  Panel Discussion </br>
                </div>
            </div>
        </div>
    </section>

    <section id="people" class="section people">
        <div class="container">
            <div class="row">
                <div class="col-md-12">
                    <h3 class="section-title">People</h3>
                    <h4>Organizers</h4>
                    <ul>

                    <li><a href="www.jian-tang.com">Jian Tang</a> (<a href="https://scholar.google.com/citations?user=1ir6WUEAAAAJ&hl=en">Google scholar</a>) </li>
                    <p>1. Email:<br> jian.tang@hec.ca
                    <br>2. Research expertise:<br> Jian is an assistant professor at HEC Montreal and also a core faculty member at Mila-Quebec AI Institute. He is a recipient of the first cohort of Canada AI Research Chairs. His main research interest is graph representation learning and graph neural networks with applications in knowledge graphs, drug discovery, and recommender systems. Before joining HEC Montreal, he was a postdoc at University of Michigan and also Carnegie Mellon University, and also a researcher at Microsoft Research Asia. He received a few best paper awards including ICML'14 Best Paper Award, WWW'16 Best Paper Nomination, and Best Paper Award at KDD'19 Workshop on Deep Learning Practice for High-dimensional Space data. He published one of the earliest work on graph representation learning---the LINE algorithm---which has been cited close to 1,900 times since it was published in 2015.  
                    <br>3. Previous experience:<br> Jian has co-organized workshops at SDM'19, CIKM'19, AAAI'20. </p>

                    <li><a href="https://www.cc.gatech.edu/~lsong/">Le Song</a> (<a href="https://scholar.google.de/citations?user=Xl4E0CsAAAAJ">Google scholar</a>) </li>
                    <p>1. Email:<br> lsong@cc.gatech.edu
                    <br>2. Research expertise:<br> Le is an Associate Professor in the College of Computing, an Associate Director of the Center for Machine Learning, Georgia Institute of Technology. My principal research direction is machine learning, especially nonlinear models, such as kernel methods and deep learning, probabilistic graphical models. Before I joined the Georgia Institute of Technology in 2011, I was a postdoc in the Department of Machine Learning, Carnegie Mellon University, and a research scientist at Google. I am also the recipient of the NSF CAREER Award’14, and many best paper awards, including the NIPS’17 Materials Science Workshop Best Paper Award, the Recsys’16 Deep Learning Workshop Best Paper Award, AISTATS'16 Best Student Paper Award, IPDPS'15 Best Paper Award, NIPS’13 Outstanding Paper Award, and ICML’10 Best Paper Award. I have served as the area chair or senior program committee for many leading machine learning and AI conferences such as ICML, NIPS, AISTATS, AAAI, and IJCAI, and the action editor for JMLR and IEEE TPAMI.
                    <br>3. Previous experience:<br> Le has co-organized workshops at ICML 2015, the Simon Institute 2017, WWW 2015, NIPS 2019, 2014 and 2012. </p>

                    <li><a href="https://cs.stanford.edu/people/jure/">Jure Leskovec</a> (<a href="https://scholar.google.com/citations?user=Q_kKkIUAAAAJ&hl=en">Google scholar</a>) </li>
                    <p>1. Email:<br> jure@cs.stanford.edu
                    <br>2. Research expertise:<br> Jure Leskovec is Associate Professor of Computer Science at Stanford University, Chief Scientist at Pinterest, and investigator at Chan Zuckerberg Biohub. His research focuses on machine learning and data mining large social, information, and biological networks. Computation over massive data is at the heart of his research and has applications in computer science, social sciences, marketing, and biomedicine. This research has won several awards including a Lagrange Prize, Microsoft Research Faculty Fellowship, Alfred P. Sloan Fellowship, and numerous best paper and test of time awards. Leskovec received his bachelor’s degree in computer science from the University of Ljubljana, Slovenia, and his PhD in machine learning from Carnegie Mellon University and postdoctoral training at Cornell University.
                    <br>3. Previous experience:<br> Jure has co-organized workshops at NeurIPS'19, ICML'19, ICLR'19, and multiple workshops at KDD and WWW.  </p>

                    <li><a href="http://www.cs.toronto.edu/~rjliao/">Renjie Liao</a> (<a href="https://scholar.google.com/citations?user=2wrS35MAAAAJ&hl=en">Google scholar</a>) </li>
                    <p>1. Email:<br> lrjconan@gmail.com
                    <br>2. Research expertise:<br> Renjie is a PhD student from the machine learning group, University of Toronto. He is jointly supervised by Raquel Urtasun and Richard Zemel. He also works part-time as a senior research scientist in Uber Advanced Technology Group. His research interest is machine learning with a recent focus on deep probabilistic models of graph-structured data and its applications. He is also interested in applying machine learning algorithms to solve various computer vision and self-driving problems. He has made several contributions in the field of graph neural networks, published at top-tier venues in the machine learning community (NeurIPS, ICLR, ICML) and in the computer vision community (CVPR, ICCV). He is the lead developer of graph recurrent attention networks, LanczosNet, NerveNet, and Graph Partition Networks.
                    <br>3. Previous experience:<br>  Renjie has co-organized related workshops at NeurIPS 2019, ICML 2019, and KDD 2019.</p>

                    <li><a href="http://www.cs.toronto.edu/~yujiali/">Yujia Li</a> (<a href="https://scholar.google.ca/citations?user=UY7CtEwAAAAJ&hl=en">Google scholar</a>) </li>
                    <p>1. Email:<br> yujiali@google.com
                    <br>2. Research expertise:<br> Yujia Li is a senior research scientist at DeepMind.  He obtained his Ph.D. from University of Toronto.  He has been working on graph neural networks since 2015 and is particularly interested in scaling up GNNs, graph generative models, and GNN's real world applications.
                    <br>3. Previous experience:<br>  Yujia has co-organized related workshops at ICML 2019 and ICLR 2019.</p>

                    <li><a href="https://www.cs.utoronto.ca/~fidler/">Sanja Fidler</a> (<a href="https://scholar.google.com/citations?user=CUlqK5EAAAAJ&hl=en">Google scholar</a>) </li>
                    <p>1. Email:<br> fidler@cs.toronto.edu
                    <br>2. Research expertise:<br> Sanja Fidler is an Assistant Professor at University of Toronto. Prior coming to Toronto, in 2012/2013, she was a Research Assistant Professor at Toyota Technological Institute at Chicago, an academic institute located in the campus of University of Chicago. She did her postdoc with Prof. Sven Dickinson at University of Toronto in 2011/2012. Sanja finished her PhD in 2010 at University of Ljubljana in Slovenia. In 2010, she visited Prof. Trevor Darrell‘s group at UC Berkeley and ICSI. She got her BSc degree in Applied Math at University of Ljubljana. Her work is in the area of Computer Vision. Her main research interests are 2D and 3D object detection, particularly scalable multi-class detection, object segmentation and image labeling, and (3D) scene understanding. She is also interested in the interplay between language and vision: generating sentential descriptions about complex scenes, as well as using textual descriptions for better scene parsing (e.g., in the scenario of the human-robot interaction).
                    <br>3. Previous experience:<br>  Previous experience: Sanja has co-organized many workshops at ICCV and ECCV. She will be the program chair of ICCV'21 and 3DV'16.</p>

                    <li><a href="http://www.cs.toronto.edu/~zemel/inquiry/home.php">Richard Zemel</a> (<a href="https://scholar.google.ca/citations?user=iBeDoRAAAAAJ&hl=en">Google scholar</a>) </li>
                    <p>1. Email:<br> fzemel@cs.toronto.edu
                    <br>2. Research expertise:<br> Richard Zemel is a Professor of Computer Science at the University of Toronto, where he has been a faculty member since 2000. Prior to that, he was an Assistant Professor in Computer Science and Psychology at the University of Arizona and a Postdoctoral Fellow at the Salk Institute and at Carnegie Mellon University. He received a B.Sc. degree in History & Science from Harvard University in 1984 and a Ph.D. in Computer Science from the University of Toronto in 1993. He is also the co-founder of SmartFinance, a financial technology startup specializing in data enrichment and natural language processing. His awards include an NVIDIA Pioneers of AI Award, a Young Investigator Award from the Office of Naval Research, a Presidential Scholar Award, two NSERC Discovery Accelerators, and seven Dean’s Excellence Awards at the University of Toronto. He is a Fellow of the Canadian Institute for Advanced Research and is on the Executive Board of the Neural Information Processing Society, which runs the premier international machine learning conference. His research contributions include foundational work on systems that learn useful representations of data without any supervision; methods for learning to rank and recommend items; and machine learning systems for automatic captioning and answering questions about images. He developed the Toronto Paper Matching System, a system for matching paper submissions to reviewers, which is being used in many conferences, including NIPS, ICML, CVPR, ICCV, and UAI. His research is supported by grants from NSERC, CIFAR, Microsoft, Google, Samsung, DARPA and iARPA.
                    <br>3. Previous experience:<br>  Previous experience: Richard is the Co-Founder and Director of Research at the Vector Institute for Artificial Intelligence.</p>


                    <li><a href="https://www.cs.cmu.edu/~rsalakhu/">Ruslan Salakhutdinov</a> (<a href="https://scholar.google.ca/citations?user=ITZ1e7MAAAAJ&hl=en">Google scholar</a>) </li>
                    <p>1. Email:<br> rsalakhu@cs.cmu.edu
                    <br>2. Research expertise:<br> Ruslan Salakhutdinov is a UPMC professor of Computer Science in the Machine Learning Department, School of Computer Science at Carnegie Mellon University. He received his PhD in machine learning (computer science) from the University of Toronto in 2009. After spending two post-doctoral years at the Massachusetts Institute of Technology Artificial Intelligence Lab, he joined the University of Toronto as an Assistant Professor in the Department of Computer Science and Department of Statistics. In February of 2016, he joined the Machine Learning Department at Carnegie Mellon University as an Associate Professor. Ruslan's primary interests lie in deep learning, machine learning, and large-scale optimization. His main research goal is to understand the computational and statistical principles required for discovering structure in large amounts of data. He is an action editor of the Journal of Machine Learning Research and served on the senior programme committee of several learning conferences including NIPS and ICML. He is an Alfred P. Sloan Research Fellow, Microsoft Research Faculty Fellow, Canada Research Chair in Statistical Machine Learning, a recipient of the Early Researcher Award, Connaught New Researcher Award, Google Faculty Award, Nvidia's Pioneers of AI award, and is a Senior Fellow of the Canadian Institute for Advanced Research. 
                    <br>3. Previous experience:<br> Ruslan has co-organized workshops at ICML'13, NeurIPS'11, NeurIPS'10, NeurIPS'09, ICML'09, NeurIPS'07, and he was the program co-chair at ICML'19.</p>

                    </ul>
                </div>
            </div>
        </div>
    </section>

    <section id="references" class="section overview">
        <div class="container">
            <div class="row">
                <div class="col-md-12">
                    <h3 class="section-title">References</h3>
                    
                    <ul>

                    <li>Alet, F., Jeewajee, A. K., Bauza, M., Rodriguez, A., Lozano-Perez, T., & Kaelbling, L. P. (2019). Graph element networks: adaptive, structured computation and memory. arXiv preprint arXiv:1904.09019.</li>

                    <li>Skryagin, A., Stelzner, K., Molina, A., & Ventola, F. SPLog: Sum-Product Logic. In International Conference on Probabilistic Programming, 2020.</li>

                    <li>Barceló, P., Kostylev, E. V., Monet, M., Pérez, J., Reutter, J., & Silva, J. P. (2019, September). The Logical Expressiveness of Graph Neural Networks. In International Conference on Learning Representations.</li>

                    <li>Battaglia, P. W., Hamrick, J. B., Bapst, V., Sanchez-Gonzalez, A., Zambaldi, V., Malinowski, M., ... & Gulcehre, C. (2018). Relational inductive biases, deep learning, and graph networks. arXiv preprint arXiv:1806.01261.</li>

                    <li>Bengio, Y. (2017). The consciousness prior. arXiv preprint arXiv:1709.08568.</li>

                    <li>Dornadula, A., Narcomey, A., Krishna, R., Bernstein, M., & Li, F. F. (2019). Visual Relationships as Functions: Enabling Few-Shot Scene Graph Prediction. In Proceedings of the IEEE International Conference on Computer Vision Workshops (pp. 0-0).</li>

                    <li>Hu, R., Rohrbach, A., Darrell, T., & Saenko, K. (2019). Language-conditioned graph networks for relational reasoning. In Proceedings of the IEEE International Conference on Computer Vision (pp. 10294-10303).</li>

                    <li>Hudson, D., & Manning, C. D. (2019). Learning by abstraction: The neural state machine. In Advances in Neural Information Processing Systems (pp. 5901-5914).</li>

                    <li>Ji, J., Krishna, R., Fei-Fei, L., & Niebles, J. C. (2019). Action Genome: Actions as Composition of Spatio-temporal Scene Graphs. arXiv preprint arXiv:1912.06992.</li>

                    <li>Khalil, E., Dai, H., Zhang, Y., Dilkina, B., & Song, L. (2017). Learning combinatorial optimization algorithms over graphs. In Advances in Neural Information Processing Systems (pp. 6348-6358).</li>

                    <li>Kipf, T., Fetaya, E., Wang, K. C., Welling, M., & Zemel, R. (2018). Neural relational inference for interacting systems. arXiv preprint arXiv:1802.04687.</li>

                    <li>Manhaeve, R., Dumancic, S., Kimmig, A., Demeester, T., & De Raedt, L. (2018). Deepproblog: Neural probabilistic logic programming. In Advances in Neural Information Processing Systems (pp. 3749-3759).</li>

                    <li>Qu, M., & Tang, J. (2019). Probabilistic logic neural networks for reasoning. In Advances in Neural Information Processing Systems (pp. 7710-7720).</li>

                    <li>Qu, M., Bengio, Y., & Tang, J. (2019). Gmnn: Graph markov neural networks. arXiv preprint arXiv:1905.06214.</li>

                    <li>Sato, R., Yamada, M., & Kashima, H. (2019). Approximation Ratios of Graph Neural Networks for Combinatorial Problems. In Advances in Neural Information Processing Systems (pp. 4083-4092).</li>

                    <li>Silver, T., Allen, K. R., Lew, A. K., Kaelbling, L. P., & Tenenbaum, J. (2019). Few-Shot Bayesian Imitation Learning with Logic over Programs. arXiv preprint arXiv:1904.06317.</li>

                    <li>Velickovic, P., Ying, R., Padovano, M., Hadsell, R., & Blundell, C. (2019). Neural execution of graph algorithms.arXiv preprintarXiv:1910.10593.</li>

                    <li>Wang, P. W., Donti, P. L., Wilder, B., & Kolter, Z. (2019). SATNet: Bridging deep learning and logical reasoning using a differentiable satisfiability solver. arXiv preprint arXiv:1905.12149.</li>

                    <li>Xu, K., Hu, W., Leskovec, J., & Jegelka, S. (2018). How powerful are graph neural networks?. arXiv preprint arXiv:1810.00826.</li>

                    <li>Yoon, K., Liao, R., Xiong, Y., Zhang, L., Fetaya, E., Urtasun, R., ... & Pitkow, X. (2018). Inference in probabilistic graphical models by graph neural networks. arXiv preprint arXiv:1803.07710.</li>

                    <li>Zhang, Y., Chen, X., Yang, Y., Ramamurthy, A., Li, B., Qi, Y., & Song, L. (2020). Efficient Probabilistic Logic Reasoning with Graph Neural Networks. arXiv preprint arXiv:2001.11850.</li>

                    </ul>
                </div>
            </div>
        </div>
    </section>

    <footer class="site-footer">
        <div class="container">
            <div class="row">
                <div class="col-md-12">
                    <p class="site-info">Designed and Developed by <a href="http://technextit.com">Technext Limited</a></p>
<!--                    <ul class="social-block">
                        <li><a href=""><i class="ion-social-twitter"></i></a></li>
                        <li><a href=""><i class="ion-social-facebook"></i></a></li>
                        <li><a href=""><i class="ion-social-linkedin-outline"></i></a></li>
                        <li><a href=""><i class="ion-social-googleplus"></i></a></li>
                    </ul>-->
                </div>
            </div>
        </div>
    </footer>

    <!-- script -->
    <script src="bower_components/jquery/dist/jquery.min.js"></script>
    <script src="bower_components/bootstrap/dist/js/bootstrap.min.js"></script>
    <script src="bower_components/smooth-scroll/dist/js/smooth-scroll.min.js"></script>
    <script src="assets/js/main.js"></script>
</body>
</html>
